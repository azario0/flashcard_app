{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f45584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "text = extract_text_from_pdf('1409.3215v3.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1dfdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import os # For environment variable (recommended for API key)\n",
    "import re\n",
    "\n",
    "def generate_flashcards_gemini_flash(\n",
    "    api_key: str,\n",
    "    text_input: str,\n",
    "    max_flashcards: int = 10,\n",
    "    temperature: float = 0.3,\n",
    "    count_tokens_before_sending: bool = True # New argument\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generates flashcard (question-answer) pairs from the given text using Gemini 1.5 Flash.\n",
    "    Optionally counts tokens before making the main generation call.\n",
    "\n",
    "    Args:\n",
    "        api_key: Your Google Generative AI API key.\n",
    "        text_input: The text from which to extract flashcards.\n",
    "        max_flashcards: The maximum number of flashcards to generate.\n",
    "        temperature: Controls the randomness of the output. Lower is more deterministic.\n",
    "        count_tokens_before_sending: If True, will count and print prompt tokens.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary has \"question\" and \"answer\" keys.\n",
    "        Returns an empty list if an error occurs or no flashcards are generated.\n",
    "    \"\"\"\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key must be provided.\")\n",
    "    if not text_input or not text_input.strip():\n",
    "        print(\"Warning: Input text is empty.\")\n",
    "        return []\n",
    "\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name='gemini-1.5-flash-latest',\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            temperature=temperature,\n",
    "        ),\n",
    "        # safety_settings can be adjusted if needed\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert flashcard creator. Your task is to analyze the following text and generate concise question-answer pairs \n",
    "    suitable for flashcards.\n",
    "    The questions should test understanding of key concepts, facts, definitions, or important relationships mentioned in the text.\n",
    "    The answers should be directly derivable from the text and be as brief as possible while still being accurate and complete.\n",
    "    Aim to generate up to {max_flashcards} flashcard pairs.\n",
    "\n",
    "    Format your output STRICTLY as a JSON list of objects. Each object must have a \"question\" key and an \"answer\" key.\n",
    "    Do NOT include any introductory text, explanations, or markdown formatting like ```json ... ``` outside of the JSON structure itself.\n",
    "    Just return the raw JSON list.\n",
    "\n",
    "    Example of expected JSON output format:\n",
    "    [\n",
    "        {{\"question\": \"What is the primary function of mitochondria?\", \"answer\": \"To generate most of the cell's supply of adenosine \n",
    "        triphosphate (ATP), \n",
    "        used as a source of chemical energy.\"}},\n",
    "        {{\"question\": \"Who developed the theory of relativity?\", \"answer\": \"Albert Einstein.\"}}\n",
    "    ]\n",
    "\n",
    "    Here is the text to process:\n",
    "    ---\n",
    "    {text_input}\n",
    "    ---\n",
    "\n",
    "    Generate the flashcards now:\n",
    "    \"\"\"\n",
    "\n",
    "    if count_tokens_before_sending:\n",
    "        try:\n",
    "            token_count_response = model.count_tokens(prompt)\n",
    "            print(f\"Estimated token count for the prompt: {token_count_response.total_tokens}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not count tokens: {e}\")\n",
    "            # Decide if you want to proceed without the count or stop\n",
    "            # For now, we'll just print a warning and continue\n",
    "            print(\"Warning: Proceeding without token count.\")\n",
    "\n",
    "    if token_count_response.total_tokens<10000:\n",
    "        try:\n",
    "            print(\"Sending request to Gemini API for generation...\")\n",
    "            response = model.generate_content(prompt)\n",
    "\n",
    "            if not response.parts:\n",
    "                print(\"Warning: Gemini API returned no parts in the response.\")\n",
    "                if response.prompt_feedback:\n",
    "                    print(f\"Prompt feedback: {response.prompt_feedback}\")\n",
    "                return []\n",
    "\n",
    "            raw_json_text = response.text\n",
    "            match = re.search(r\"```(json)?\\s*([\\s\\S]*?)\\s*```\", raw_json_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                cleaned_json_text = match.group(2)\n",
    "            else:\n",
    "                cleaned_json_text = raw_json_text.strip()\n",
    "\n",
    "            if cleaned_json_text and not (cleaned_json_text.startswith('[') and cleaned_json_text.endswith(']')):\n",
    "                print(f\"Warning: Gemini API response does not look like a JSON list. Raw response:\\n{raw_json_text}\")\n",
    "                match_list = re.search(r\"(\\[[\\s\\S]*\\])\", cleaned_json_text)\n",
    "                if match_list:\n",
    "                    cleaned_json_text = match_list.group(1)\n",
    "                else:\n",
    "                    print(\"Error: Could not extract a valid JSON list structure from the response.\")\n",
    "                    return []\n",
    "\n",
    "            flashcards = json.loads(cleaned_json_text)\n",
    "\n",
    "            if not isinstance(flashcards, list):\n",
    "                print(f\"Error: Parsed JSON is not a list. Got: {type(flashcards)}\")\n",
    "                return []\n",
    "            for item in flashcards:\n",
    "                if not (isinstance(item, dict) and \"question\" in item and \"answer\" in item):\n",
    "                    print(f\"Error: Invalid flashcard item format: {item}\")\n",
    "                    return []\n",
    "\n",
    "            print(f\"Successfully generated {len(flashcards)} flashcards.\")\n",
    "            return flashcards\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error: Could not decode JSON from Gemini response: {e}\")\n",
    "            print(f\"Raw response from Gemini:\\n---\\n{response.text if 'response' in locals() and hasattr(response, 'text') else 'No response object or text available'}\\n---\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            if 'response' in locals() and hasattr(response, 'prompt_feedback'):\n",
    "                print(f\"Prompt feedback: {response.prompt_feedback}\")\n",
    "            return []\n",
    "    else :\n",
    "        print('too much tokens limit reached !')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91bdd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_API_KEY = 'YOUR_API_KEY'\n",
    "if not MY_API_KEY:\n",
    "    print(\"Error: GEMINI_API_KEY environment variable not set.\")\n",
    "    print(\"Please set it before running the script, e.g.:\")\n",
    "    print(\"export GEMINI_API_KEY='your_actual_api_key'\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\n--- Flashcards for Photosynthesis Text ---\")\n",
    "flashcards1 = generate_flashcards_gemini_flash(MY_API_KEY, text, max_flashcards=15)\n",
    "if flashcards1:\n",
    "    for i, card in enumerate(flashcards1):\n",
    "        print(f\"\\nQ{i+1}: {card['question']}\")\n",
    "        print(f\"A{i+1}: {card['answer']}\")\n",
    "else:\n",
    "    print(\"No flashcards generated for Photosynthesis text.\")\n",
    "\n",
    "    print(\"\\n--- Flashcards for Industrial Revolution Text ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22551187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the main contribution of this paper?',\n",
       "  'answer': 'A general end-to-end approach to sequence learning using LSTMs, achieving state-of-the-art results on English-to-French translation.'},\n",
       " {'question': 'What architecture is used in this sequence-to-sequence learning model?',\n",
       "  'answer': 'Multilayered Long Short-Term Memory (LSTM) networks.'},\n",
       " {'question': 'What dataset was used for the English-to-French translation task?',\n",
       "  'answer': 'WMT’14 dataset.'},\n",
       " {'question': \"What was the LSTM's BLEU score on the WMT’14 English-to-French translation task?\",\n",
       "  'answer': '34.8'},\n",
       " {'question': 'What was the BLEU score of the phrase-based SMT system on the same dataset?',\n",
       "  'answer': '33.3'},\n",
       " {'question': \"How did reversing the source sentence order affect the LSTM's performance?\",\n",
       "  'answer': 'It improved performance markedly by introducing short-term dependencies.'},\n",
       " {'question': 'What is a significant limitation of Deep Neural Networks (DNNs)?',\n",
       "  'answer': 'They can only be applied to problems with fixed-dimensionality inputs and outputs.'},\n",
       " {'question': 'What is the role of the first LSTM in the proposed model?',\n",
       "  'answer': 'To map the input sequence to a fixed-dimensional vector.'},\n",
       " {'question': 'What is the role of the second LSTM in the proposed model?',\n",
       "  'answer': 'To decode the target sequence from the fixed-dimensional vector.'},\n",
       " {'question': 'How many parameters did the deep LSTM model have?',\n",
       "  'answer': '384M'},\n",
       " {'question': 'What technique was used to improve the BLEU score further?',\n",
       "  'answer': 'Rescoring the 1000-best hypotheses from an SMT system.'},\n",
       " {'question': 'What was the improved BLEU score after rescoring?',\n",
       "  'answer': '36.5'},\n",
       " {'question': 'What is the size of the vocabulary used for the target language (French)?',\n",
       "  'answer': '80,000'},\n",
       " {'question': 'What was the primary training objective?',\n",
       "  'answer': 'Maximizing the log probability of a correct translation given the source sentence.'},\n",
       " {'question': \"What simple data transformation significantly improved the LSTM's performance?\",\n",
       "  'answer': 'Reversing the order of words in the source sentences.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flashcards1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1917c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
